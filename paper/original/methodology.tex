\section{Methodology}
\label{sec:meth}

Given the defined research questions and related work to this study, it needs to adhere to the basic steps of forecasting \cite{hyndman2018forecasting, davydenko2021forecast}:

\begin{itemize}
    \item[Data]: Information gathering and EDA
    \item[Models]: Choosing and fitting models
    \item[Evaluation]: Model evaluation methods
\end{itemize}

To determine the extent of influence of COVID-19 pandemic data on stock market pricing, all factors in the questions are isolated and answered separately. The effect of the data can be determined by using comparisons in correlation scores between features or non-parametric tests. This gives a reliable result for one-on-one relationships between explanatory and response variables, but this becomes a lot more complex in multivariate analysis. This study proposes an alternative solution by comparing forecasting performance using a dynamic model framework where feature importance is compared by applying the mean absolute percentage error (MAPE) on a selection of machine learning models, isolating features between comparisons.

$$\mbox{MAPE} = \frac{100}{n}\sum_{t=1}^n  \left|\frac{A_t-F_t}{A_t}\right|$$

To determine the features to be tested, a couple of choices concerning the available data surrounding the COVID-19 pandemic \ref{sec:data} have to be made. As the performance of the data is also dependent on the model trained with it, a collection of models will account for linear- and non-linearity. Statistical dependency can be accounted for by comparing model scores of a LR model with an ARIMA model, as the former assumes statistical dependency \ref{sec:models} and the latter does not. To evaluate performance of the models and data, it is necessary to methodically isolate different parts of the research question given the MAPE metric.

\subsection{Data: Information gathering and EDA}
\label{sec:data}

To fulfill the data requirements, this research is based on a small set of sources:

\begin{itemize}
    \item Calendar data, such as day of the week, weeks, months, years, etc. Its scope will go up to daily data, as that is the finest granularity between all historical data sources.
    \item Weather data, as it has small but noticeable effects on human behaviour. This study will focus its efforts on the Dutch market, where the Royal Netherlands Meteorological Institute (KNMI) is the authority on Dutch weather \footnote{KNMI - Royal Netherlands Meteorological Institute: https://www.knmi.nl/home}, with The Bilt (Utrecht) as the central point of measurements.
    \item Stock market data, collected from the Yahoo Finance API \cite{aroussi_2017}, with the 25 companies representing the Amsterdam Stock Exchange (AEX) index as a representation of the Dutch stock market. As the AEX index is evaluated each quarter, it is assumed as a static composition of companies as of 30th June 2021 \cite{aexindex2021}.
    \item COVID-19 data, provided by
    \begin{itemize}
        \item the Dutch institute of national well-being and environment (RIVM), providing official information on COVID-19 infections and deaths in the Netherlands.
        \item the Johns Hopkins University repository for COVID-19 data \cite{dong2020interactive}, a global dataset providing insight into infections and deaths, but also vaccinations, currently (as of June 2021) not officially provided by the Dutch authorities.
        \item the Oxford COVID-19 Government Response Tracker \cite{hale2020variation}, a global tracker of government responses to the COVID-19 pandemic, including lockdown, social distancing and business measures.
    \end{itemize}
\end{itemize}

% As we are dealing with stock market data, which is notoriously volatile, there are multiple data sources that could aid predictions. Speculation on news publications, especially related to business development of publicly traded stocks, is fairly common in stock exchanges. This data is highly unstructured and requires a lot of manipulation for it to be of value to our models. Therefore we do not include this data, as it complicates model development heavily and could theoretically introduce a lot of noise.

Because of the variety of data sources, it's not likely all data is perfectly clean and distributed for this study. To ensure the models treat all features equally, data has to be preprocessed. During EDA all features were investigated and potential problems were identified with the data before collecting representative results to test our research questions.

\subsubsection{Preprocessing of calendar data}

Calendar data can be generated and as this study models time-series data, it serves as the connection for all other features. It also has its own characteristics which could aid in distinguishing patterns in the data, such as weekdays, boundaries of weeks, months or years and the distinction between workdays and weekends. During EDA it was concluded that these features are counter-productive for the models, as they are discrete variables but produce artifacts at the boundary between periods that do not represent reality. For instance, week 52 is a later than week 1, except at the transition of two years. This could be resolved by counting from an arbitrary starting point in the data and only counting up, but generally this just produces more features that are confusing to the models. The days of the week are different, as these are more categorical in nature rather than discrete variables. For instance when every Monday is counted incrementally, it would represent the number of weeks rather than every Monday. Therefore its better to one-hot encode weekdays.

% TODO: Check whether incremental discrete variables for day/week/month/year do work
% Not sure is this makes sense

\subsubsection{Preprocessing of weather data}

Weather data consists of a few basic variables: air humidity and -pressure, wind speed, cloud coverage and conversely hours of sunshine, precipitation and temperature. All these features have their respective minimum and maximum values with the time-slot where that value was measured and an average or total sum depending on the type of variable. As the granularity of the data does not allow for hourly intervals and extreme values might badly influence model training, this study restricts the weather features to averages and totals of the main features mentioned.

\subsubsection{Preprocessing of stock market data}

Stock market data consists of two basic variables: pricing and volume. Pricing is generally represented by a opening- and closing price, a high and a low value per day. In stock trading average values tend to be less informative, as it is the extreme differences in value between certain periods that are of value for stock trading. There are numerous metrics that exist within economics and financial theory that claim to be good indicators of the actual value of an asset, but only open, close, high and low values along with volume are agreed upon as being representative indicators. As all other constructions of these indicators are open to speculation, this study treats these values individually as predictor values.

A different challenge with stock market data is that it is not always available, as a stock market is not always open. Weekends, holidays and other periods might not contain data, so with preprocessing these are indicated as a variable in the dataset. As zero-filling is not representative of these variables, it is assumed that the value of prices or volume remain static during these periods. This has its dangers as the world does not 'switch off' during these periods and speculation on share prices will still occur, without it being reflected in actual trading prices. Although recording the closing days of the stock market might help models detect and predict these periods, it will inevitably introduce noise to the predictions and it might be necessary to move toward a weekly prediction interval rather than a daily interval.

\subsubsection{Preprocessing of pandemic data}

COVID-19 data features four variables: infected, deceased and vaccinated people, along with government measures. All these can either be viewed as daily recorded values or cumulative values, with the exception infected people, as the duration of COVID-19 infection varies, generally showing a two week average duration. Data is provided by two sources, the RIVM and the JHU, where the former can be viewed as reliable and the latter can be changeable, as the data is crowd-sourced. Both suffer from the flaw that the recording of a value might differ from the actual value at that time. Infected people might not always know they are infected and their infection status is only recorded if they decide to be officially tested. A similar phenomenon occurs with the number of deceased people, as the cause of death may not always be determined (to a single cause), people are not required to notify the government and the deceased are recorded by this notification date and not time of death. Vaccination data features these same inconsistencies, but also has the deficit of being crowd-sourced from various sources at the start of recording, as the RIVM did not officially record these statistics at the start of this study. Therefore this study will rely on the vetting mechanism that open-source data applies, accepting any errors this may introduce.

Some gaps in the data may occur and with cumulative values a 'steady state' is assumed, meaning no change is expected in the state unless recorded. For daily recorded statistics this means data is zero-filled if no value is known. This differs from the approach with stock market data, as vaccinations could be halted due to political or medical re-evaluations.\footnote{The AstraZeneca and Janssen vaccines were both found to cause embolic/thrombotic events in patients and were suspended until safety was reassessed: \\ \url{https://www.cdc.gov/vaccines/acip/meetings/downloads/slides-2021-01/02-COVID-Villafana.pdf}\\  \url{https://www.fda.gov/news-events/press-announcements/joint-cdc-and-fda-statement-johnson-johnson-covid-19-vaccine}}
Although it can be assumed infections and deaths maintain relatively stable figures, this study is aware that this data is curated and historically corrected, removing the necessity for gap-filling.

All data is prepared in a similar fashion before model training and evaluation. Because it can not be assumed that features are homogeneous, min-max-normalization is applied on the data between all transformations so that each feature is scaled identically without modifying their distribution. As for transforming the distribution of non-normally distributed data, it was concluded from EDA that applying a log-transformation, a second re-scaling along with a Yeo-Johnson transformation \cite{yeo2000new}, yields the best results for data distribution, while maintaining distribution characteristics for normally distributed data. Finally the data is re-scaled once more before model training so each feature will be weighted similarly. As the response variable this study chose to look at opening prices of stocks, as a single variable allows for easier error metrics. From EDA it was concluded that there is little difference in either opening, closing, high or low pricing values.

\subsection{Feature selection}

As the principal of 'simple is better' generally applies to time-series forecasting, it can not be assumed that all features combined as the full training set is best. To remove these assumptions, recursive feature elimination is implemented during model training based on the mean squared error (MSE) score of a prediction with five fold cross validation. However, this only applies for the LR and RFR models as the used ARIMA model implementation does not allow for automatic feature selection.\footnote{pmdarima: ARIMA estimators for Python \url{https://alkaline-ml.com/pmdarima/index.html}} For the ARIMA model a Fourier Featurizer is implemented to fit seasonal as well as non-seasonal features effectively. And lastly our baseline model does not necessitate any feature selection as it disregards all input features other than the last value of the response variable, which results as a constant for each training cycle.

\subsection{Models: Choosing and fitting models}
\label{sec:models}

To accommodate a valid conclusion to the research questions, a variety of models must be considered  and compared in their individual performances to the larger research questions. Only then can this study conclude if it is the model that accounted for a gain in performance.

\subsubsection{Baseline model}

For our baseline model, it is necessary to ensure the performance is consistent irrespective of the input data. Effectively this means the baseline model should predict a constant value from the last known value in a time-series. Assuming the error of the predicted values is normally distributed, the MAPE metric should be relatively consistent over multiple predictions and inputs.

$$\hat{Y}_{T+h|T} = Y_T$$

For every time-step $t$ and feature $i$, we define the predictor $y^{t+1}$ as the last time-step $t$ of input value $x$.

\subsubsection{Linear regression}

As the standard for most prediction models, a model based on multiple linear regression (LR) is implemented. As the model expects constant variance, independence of errors and linearity of the data, it will only perform adequately if these conditions are met. With specific transformations applied to the data to improve model fitting, it serves as a good comparison to the other models as its value has been proven in many fields and disciplines. 

\subsubsection{ARIMA}

As a more advanced regression model, ARIMA has the potential to fit the data quite well. As an auto-regressive (AR) model, it does not assume independent data and serves as a good comparison to LR on this feature of the model. The moving average (MA) accounts for the errors in the AR part of the model, improving the model fitting. Still, this ARMA model does not work on non-stationary data, which is where the (I) integrated part of the model comes in. This accounts for the difference in time-steps ($Y_t+1 - Y_t$) in the data and transforms the series so it does become stationary for the ARMA part to work. The key takeaway being that the three parameters $p$, $d$ and $q$, respectively representing AR, I and MA, need to be tuned for a correct model fitting. This can be performed automatically by performing differencing tests for the $p$ parameter and iterating step-wise through the $d$ and $q$ parameters. This could lead to over-fitting the model, but with multiple samples of the data this study should get representative model performance.

\subsubsection{Random Forest Regressor}

As a state-of-the-art (SOTA) model a random forest regressor (RFR) is implemented. It can fit non-linear data, but is a 'black-box' model, so compared to LR and ARIMA its inner workings are not fully intuitive. But where it lacks in explainability it gains in performance. Related literature also points out that RNN/LSTM networks can perform well on stock market time-series data \cite{YU20082623}, but these networks require quite large amounts of data compared to an RFR model. Therefore it is the best machine learning model we could implement on our data.

\subsection{Evaluation: Model evaluation methods}
\label{sec:eval}

To evaluate model performance it is compared between different segments of the research questions using the MAPE metric. In these questions the following components can be isolated:

\begin{itemize}
    \item Model performance
    \item The time period of the sample
    \item Feature performance
\end{itemize}

Using these cross-sections of the research questions a statistical hypothesis test can be performed on the MAPE metric to draw a valid conclusion. When a model will not significantly outperform the baseline model, this study has to acknowledge that the research question does not have a conclusive answer given the data and models.