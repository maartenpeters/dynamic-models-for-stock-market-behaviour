\section{Related Work}
\label{sec:rel}

% Deze sectie bestaat uit een aantal "blokken", waarin je per blok de relevante literatuur beschrijft. 

% Neem alleen literatuur op die van belang is voor jouw onderzoeksvraag en deelvragen.

% Typisch heb je 1 blok voor je hoofdvraag en per deelvraag \textbf{RQi} een blok. 

As this thesis looks at the influence of COVID-19 pandemic data on stock market values, a couple of sections are dedicated to discuss related work. As the research questions are related to models and data, these are discussed as separate topics.

\subsection{Data}

\subsubsection{Pandemic Data}

As the COVID-19 pandemic began to increase in number of cases, world-wide spread and severity, a large part of the academic world focused their efforts on solving issues related to the pandemic or contributing to the cause in general. One of the most influential parties was the Johns Hopkins University's (JHU) with their Coronavirus Research Center (CRC) and their Center for Systems Science and Engineering (CSSE) \cite{dong2020interactive}. They started one of the first and most successful data collections around the COVID-19 pandemic and provided world-wide figures into the number of infections, deceased and (in a later stage) vaccinations, for varying degrees of granularity. In the early stages of the pandemic, the focus was much more regional \cite{HUANG2020497, wang2020novel, velavan2020covid} and built around statistics such as reproductive numbers, health effects and possible treatments. As the effects of COVID-19 on the community became more apparent and the severity of the pandemic increased, governments became concerned on its effect on society and if their could be economic consequences.

It became clear that lockdown measures were necessary to reduce the number of infections. The exact measures differed from country to country, as the effectiveness of measures were always a matter of debate. Societal measures ranged from curfews mandating people to stay at home and reduce non-essential traveling, mandatory social distancing assuring people refrain from physical contact \cite{lewnard2020scientific} or the wearing of face masks in public spaces to reduce the spread of airborne viral particles \cite{eikenberry2020}. Businesses also faced several restrictions, as the hospitality and tourism industries were forced to close, as they could become hot-spots for disease transmission \cite{gossling2020pandemics}. This in turn also affected the air-travel industry, as people were recommended to stay home and borders were closed off for international travel. And lastly, any other business were often required to provide means for personnel to work remotely from their homes, as offices could also become hot-spots for COVID-19 transmission \cite{dingel2020many}. As these measures varied, especially when each country developed infection peaks at different times, there became a need for COVID-19 related research to have a unified way of charting these measures. The Blavatnik School of Government, part of the University of Oxford, provides a centralized dataset that collects and standardizes the measures taken by countries to reduce the effect of the COVID-19 pandemic \cite{hale2020variation}. They provide categories such as containment and closure policies, economic policies, health system policies, vaccination policies and miscellaneous policies over which they proposed a stringency index that indicates the severity of the measures applied.

\subsubsection{Environmental Data}

As this study revolves around effects on stock market data, environmental factors can not be excluded. Although stock market data are very volatile, their variance can be explained as results from human behaviour. And as humans are affected by a variety of physical and psychological effects, these can be attributed to environmental factors, such as weather \cite{hirshleifer2003good}. For example, without going into the principals behind causality, one might argue that there is a causal relationship between sunny weather and ice-cream sales, as people tend to buy more ice-cream during sunny (summer) days. This is an example of human behaviour being influenced by environmental factors, a phenomenon also influencing decisions of traders on the stock market. Still, human behaviour on the stock market can be erratic as people have to deal with uncertainty \cite{tversky1974judgment} and can overreact to news \cite{de1985does}.

With weather data being location bound, publicly available and easily quantifiable, it serves as an ideal candidate for environmental factors in this study. It does come with some caveats, such as it not always being available historically or not being reliable as some data providers rely on community driven data. Another issue is that weather data is localized, as compared to stock market data, which allows international trade. Still an effect might be noticeable and it serves as a good candidate for correlation and hypothesis testing when compared to pandemic data.

Another large factor in stock market data might be stock related news \cite{tversky1974judgment, veronesi1999stock}. The difficulty of news articles is that they're not easily quantifiable, unless they have been labeled for the specific use case. Unsupervised machine learning methods can be applied to perform sentiment analysis and topic modelling, but this requires large amounts of data and compute resources with low certainty of added value. Textual data also tends to contain a lot of noise, so it requires extensive cleaning before it can be used appropriately. Therefore this research will forego using stock market related news as input data, as it might interfere with the other variables in the research questions.

Another factor in the research questions is the use of calendar data. As stock markets tend to close during weekends and holidays, this data might aid in identifying seasonal patterns. This research will focus on daily and weekly data, as this is the highest frequency data available for stock markets without the use of paid data providers. Pandemic data is also provided on a daily interval, so this coincides fittingly with the other datasets.

% Related work to my thesis
% we discussed the 
% - influence of weather
% - the influence of the pandemic 
% the question remains now
% - What work is related to approach this specific problem
% What does literature say on this?


\subsection{Forecasting models}

As the COVID-19 pandemic is a recent phenomenon, studies modelling economic development after the pandemic have been performed but not yet proven \cite{chudik2020economic, baldwin2020economics, fernandes2020economic}. There have been efforts on modelling historic pandemics \cite{osterholm2017preparing, correia1918pandemics, jorda2020longer}, but most of these studies focus on long-term macroeconomic effects, such as gross domestic product (GDP), unemployment, individual/national income, consumption or inflation, looking at years or decades of development. As economic development itself is a slow process, influence by relatively short-term factors such as the stock market is a non-trivial matter. As modern technology along with the COVID-19 pandemic provides a unprecedented amount of data \cite{dong2020interactive, hale2020variation}, some research has been done on the short-term development and effects of the pandemic \cite{zhao2020preliminary, deb2020economic, carpi2021twitter}.

% To model our data on a daily frequency, we have to look at models that can perform regression tasks. As the data might be linear or non-linear, we have to account for both situations. As a baseline model, we will use a static model that uses the last known value in the response variable as a future prediction. This model will perform consistent regardless of the trend in the data.

A common practice in the development and selection of forecasting models is to perform exploratory data analysis (EDA) and select the model based on an error metric and/or subjective knowledge of the specific domain. Econometric and statistical models generally differ between these disciplines, as the former looks more at domain knowledge and adjusts the model accordingly to reduce error, while the latter relies on statistical methods to reduce model errors. As stock market data is quite volatile, using either of these disciplines can lead to a successful forecasting model, but the theoretical basis differs. As the discipline of data science and machine learning is in essence an evolution of statistics, its methods rely more on error metrics, even though a lot of gains can be made with problem specific model tuning. This study will look solely at the error metrics of models to determine the effectiveness of the model in an automatic manner.

As a baseline comparison, this study has developed a naive model that assumes no trends or variation in the response variable and forecasts the last known value of a time-series. Given the predictor variable will realistically increase or decrease with every time-step $t$, it is assumed it will always show an error, but it will average out over multiple time-series, given normalized data and a relative error metric.
% reference from
% conference
% paper
% book
% github...

% news from conferences/knowledge from books


\subsubsection{Linear model}

The discipline of forecasting goes back to the early 1700s with the prediction of the return of Halley's comet, where orbital mechanics predicted that the comet returned every 76 years. Decades later mathematicians and physicists Legendre, Gauss and Quetelet developed this theory into the basics of linear regression, the standard for most forecasting problems \cite{stigler1986history}. Linear regression (LR) assumes constant variance, independence of errors and linearity of the data. As the data can be transformed to better fit these characteristics, it has been the basic model for most of econometrics \cite{greene2003econometric}. Along with its simplicity, and its computational efficiency, it therefore serves as a good comparison to the baseline model.

\subsubsection{Random Forest Regressor}

Statistical models, although occasionally quite complex, can be explained as a mathematical relationship between explanatory and response variables. Machine learning algorithms, although mathematically motivated, tend to be less clear on their inner workings by being a function of the training data. They tend to work in non-linear situations by combining (relatively) simple mathematical models into ensembles, creating a larger non-linear model, forming the basis of a discipline known as deep learning \cite{lecun2015deep, goodfellow2016deep}. Although most machine learning models require a lot of data to be trained, a selection of models has been made available in a Python library to ease machine learning development \cite{sklearn2012}. One of the models that performs relatively well on small amounts of data is the Random Forest Regressor. A Random Forest is an random ensemble of shallow decision trees, bagged together to create a single model, capable of classification or regression tasks. Properly trained and implemented, they can achieve high accuracy, with the trade-off of lacking interpretability. Where related research implemented neural networks \cite{thawornwong2004forecasting}, these require significant tuning for good performance. Because this study looks at comparing models over a variety of data, this rules out implementing RNNs/ANNs for the sake of generalization.

\subsubsection{ARIMA}

As linear regression can be seen as a basic implementation of regression, the ARIMA model is the most robust regression model that can be used in this context \cite{siaminamini2018forecasting, chatfield2019analysis}. It also allows for automatic fitting which tries to find an optimal fit by stepping through its parameters \footnote{pmdarima: ARIMA estimators for Python: \url{https://alkaline-ml.com/pmdarima}}, although this might lead to over- or underfitting.